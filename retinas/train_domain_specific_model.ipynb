{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# IMPORT REQUIRED LIBRARIES\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# 1. Import raw text and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating tokens, saved in variable 'tokens'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take a source of text and create a python list of all the words.\n",
    "The words retain the same sequence and frequence as in original text.\n",
    "INPUT:\n",
    "    data_source, text file, Content will be parsed for sentences. This should be a plain text document\n",
    "\n",
    "OUTPUT:\n",
    "    tokens, list<list>, list of lists containing a one to one mapping of all the words in the provided text.\n",
    "\"\"\"\n",
    "view_tokens=False\n",
    "data_source = 'data/medical_docs_content.txt'\n",
    "\n",
    "\n",
    "#open input file and tokenize\n",
    "tokens = []\n",
    "with open(data_source, 'r') as source:\n",
    "    for line in source:\n",
    "        line = re.sub(r'\\W', ' ', line) \n",
    "        line_tokens = line.split( )\n",
    "        line_tokens = [token.strip().lower() for token in line_tokens]\n",
    "        tokens.append(line_tokens)\n",
    "\n",
    "if (view_tokens):\n",
    "    for idx in range(2):\n",
    "        print tokens[idx]\n",
    "\n",
    "print \"DONE creating tokens, saved in variable 'tokens'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# 2. Create a word2vec model from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model saved to models/medical_docs_content\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: Natural Language Processing in Action, Chapter 6\n",
    "Section: 6.5.2 Training domain specific word2vec model\n",
    "INPUT:\n",
    "    tokens, python list, Tokenized data from previous step.\n",
    "    model_path, string, path where we will saved trained model from tokens (depends on last steps data_source)\n",
    "OUTPUT:\n",
    "    model_path, <File on disk> a word2vec trained model from the original tokens.  (hidden weights only)\n",
    "\"\"\"\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model_path  = \"models/{}\".format(re.compile(r'\\..*').sub('', os.path.basename(data_source)))\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    tokens,              # Our array of sentences, each of which is an array of words.\n",
    "    min_count=3,         # Min number of word count to be considered\n",
    "    workers=4,           # Number of threads in parallel. (cores on laptop)\n",
    "    size=300,            # The number of weights in hidden layer, (length of word verctors)\n",
    "    window=6,            # Context window size\n",
    "    sample=1e-3          # subsampling rate for frequent terms\n",
    ")\n",
    "\n",
    "# Save disk space by saving only hidden neurons.  (We lose output weights)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print \"New model saved to {}\".format(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# 3. Import model and test it by looking for similar words.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: ['medical']\n",
      "Negative: []\n",
      "             0         1\n",
      "0       almost  0.820889\n",
      "1     practice  0.757029\n",
      "2          gyn  0.753862\n",
      "3   electronic  0.745408\n",
      "4    pediatric  0.720835\n",
      "5        using  0.715478\n",
      "6    collector  0.712447\n",
      "7      billing  0.705796\n",
      "8  terminology  0.692594\n",
      "9       athena  0.691293\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load a previously saved word2vec model and use vector math to find words deemed 'similar'\n",
    "\n",
    "INPUT:\n",
    "    model_path: string, path to file containing previously trained set of word2vec vectors.\n",
    "OUTPUT:\n",
    "    positive: list<string>, a list of words to find similar words to.\n",
    "    negavite: list<string>, a list of words whose vectors get subtracted before finding similarity\n",
    "\n",
    "Output is displayed to console.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# INTPUTS:\n",
    "positive = ['medical']\n",
    "negative = []\n",
    "\n",
    "model = word2vec.Word2Vec.load(model_path)\n",
    "results = model.most_similar(positive=positive, negative=negative, topn=20)\n",
    "# print results\n",
    "data = pd.DataFrame(results)\n",
    "\n",
    "print \"Positive: {}\".format(positive)\n",
    "print \"Negative: {}\".format(negative)\n",
    "print data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# 4. Create a 2D map from the vectors in the saved model.  (Retina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 6,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models/medical_docs_content\n",
      "Saving 2d numpy array from word_vectors to models/vectors_array\n",
      "(6994, 300)\n",
      "Training:\n",
      "sigma: 5 learning_rate: 0.8 train_iterations: 200\n",
      "Saving models/som_weights_3\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section creates the semantic map from the hidden layer vectors.\n",
    "\t1. Open a pretrained vector set 'data_for_classification_model'\n",
    "\t2. collect all vectors into one single 2d numpy array\n",
    "\t3. pass 2d array to minisom and train a map\n",
    "\t4. Save the map to som_map\n",
    "    \n",
    "INPUTS:\n",
    "    model_path, string, path to file for previously trained model from raw text\n",
    "    vectors_path, string, path to file where only numerical vectors will be saved to pass to minisom\n",
    "    weights_path, string, path to map weights defining the trained 2d minisom\n",
    "    b_collect_new_vectors, boolean, recollect the vector file (if we updated the model)\n",
    "    b_save_new_som_weights, boolean, recalculate and overwrite the weights for the 2d map.\n",
    "    training inputs:\n",
    "    _sigma: float\n",
    "    _learning_rate: float\n",
    "    _train_iterations: int\n",
    "OUTPUT:\n",
    "    Depending on the booleans, the files at vectors_path and weights_path will be overwritten or created.\n",
    "\"\"\"\n",
    "from minisom import MiniSom\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "import numpy as np \n",
    "\n",
    "#INPUTS\n",
    "b_collect_new_vectors = True\n",
    "b_save_new_som_weights = True \n",
    "vectors_path = 'models/vectors_array'\n",
    "weights_path = 'models/som_weights_3'\n",
    "# TRAINING INPUTS\n",
    "_sigma = 5\n",
    "_learning_rate = 0.8\n",
    "_train_iterations = 200\n",
    "#ENDINPUTS\n",
    "\n",
    "\n",
    "print \"Loading model {}\".format(model_path)\n",
    "word_vectors = KeyedVectors.load(model_path)\n",
    "model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "if (b_collect_new_vectors):\n",
    "    print \"Saving 2d numpy array from word_vectors to {}\".format(vectors_path)\n",
    "    varrs = np.array([word_vectors[word] for word in word_vectors.wv.vocab.keys()])\n",
    "    np.save(vectors_path, varrs)\n",
    "else:\n",
    "    print \"Loading {}.py word vectors from disk\".format(vectors_path)\n",
    "    varrs = np.load(\"{}.npy\".format(vectors_path))\n",
    "\n",
    "print varrs.shape\n",
    "som = MiniSom(x=128, y=128, input_len=300, sigma=_sigma, learning_rate=_learning_rate)\n",
    "som.random_weights_init(varrs)\n",
    "if (b_save_new_som_weights):\n",
    "    print \"Training:\"\n",
    "    print \"sigma:\", _sigma, \"learning_rate:\", _learning_rate, \"train_iterations:\",_train_iterations\n",
    "    som.train_random(varrs, _train_iterations) # random training\n",
    "\n",
    "    print \"Saving {}\".format(weights_path)\n",
    "    np.save(weights_path, som.weights)\n",
    "\n",
    "\n",
    "            \n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a list of word 'fingerprints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n",
      "13210\n",
      "valuethis\n",
      "valuethis not in vocabulary\n",
      "position\n",
      "[('endeavor', 0.8360309600830078), ('this', 0.8241738080978394), ('funded', 0.8029907941818237), ('requires', 0.7990128993988037), ('exempt', 0.76841139793396)]\n",
      "(71, 123)\n",
      "will\n",
      "[('would', 0.803634762763977), ('considered', 0.7921169996261597), ('successful', 0.7566119432449341), ('candidate', 0.7432190775871277), ('should', 0.7295222282409668)]\n",
      "(95, 47)\n"
     ]
    }
   ],
   "source": [
    "print len(tokens)\n",
    "unique_words = []\n",
    "for line_list in tokens:\n",
    "    for word in line_list:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "print len(unique_words)\n",
    "\n",
    "som.weights = np.load(\"{}.npy\".format(weights_path))\n",
    "\n",
    "\n",
    "stop = 2\n",
    "for word in unique_words:\n",
    "    print word\n",
    "    try:\n",
    "        print model.most_similar(positive=[word], negative=[], topn=5)\n",
    "        print som.winner(word_vectors[word])\n",
    "    except:\n",
    "        print word, \"not in vocabulary\"\n",
    "    stop -= 1\n",
    "    if stop < 0:\n",
    "        break\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word to location dictionary\n",
      "Number of unique words is 13210\n",
      "Completed,word_to_loc is 6994\n",
      "Completed fingerprints with length 6994\n",
      "[(47, 27), (125, 50), (75, 113), (47, 27), (125, 56), (63, 93), (45, 36), (117, 69), (122, 73), (116, 74), (126, 68)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Retina:\n",
    "    def __init__(self, _unique_words, _trained_som, _word_vectors, _model):\n",
    "        self.unique_words = _unique_words\n",
    "        self.som = _trained_som\n",
    "        self.word_vectors = _word_vectors\n",
    "        self.model = _model\n",
    "        self.word_to_loc = {}\n",
    "        self._fingerprints = {}\n",
    "        self._generate()\n",
    "\n",
    "    def _generate(self):\n",
    "        print \"Generating word to location dictionary\"\n",
    "        print \"Number of unique words is {}\".format(len(self.unique_words))\n",
    "        for word in self.unique_words:\n",
    "            try:\n",
    "                self.word_to_loc[word] = self.som.winner(self.word_vectors[word])\n",
    "            except:\n",
    "                pass\n",
    "        print \"Completed,word_to_loc is {}\".format(len(self.word_to_loc))\n",
    "\n",
    "        for word, loc in self.word_to_loc.iteritems():\n",
    "                self._fingerprints[word] = [loc]\n",
    "                similar_words = self.model.most_similar(positive=[word], negative=[], topn=10)\n",
    "                [(self._fingerprints[word]).append(self.word_to_loc[tup[0]]) for tup in similar_words] \n",
    "        print \"Completed fingerprints with length {}\".format(len(self._fingerprints))\n",
    "        print self._fingerprints['medical']\n",
    "\n",
    "            \n",
    "    def location(self, word):\n",
    "        return self.word_to_loc[word]\n",
    "\n",
    "    def fingerprint(self, word):\n",
    "        return self._fingerprints[word]\n",
    "\n",
    "    def fingerprint_x_y(self, word):\n",
    "        xarr = []\n",
    "        yarr = []\n",
    "        for tup in self.fingerprint(word):\n",
    "            xarr.append(tup[0])\n",
    "            yarr.append(tup[1])\n",
    "\n",
    "        return (xarr, yarr)\n",
    "    \n",
    "\n",
    "retina = Retina(unique_words, som, word_vectors, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([47, 125, 75, 47, 125, 63, 45, 117, 122, 116, 126],\n",
       " [27, 50, 113, 27, 56, 93, 36, 69, 73, 74, 68])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print retina.location('medical')\n",
    "print \"---------\"\n",
    "print retina.fingerprint('medical')\n",
    "print \"---------\"\n",
    "print retina.fingerprint_x_y('medical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def import_from_data_file(unique_id):\n",
    "    with open(data_source, 'r') as source:\n",
    "        csvfile = csv.reader(source)\n",
    "        for line in csvfile:\n",
    "            binary = line[6]\n",
    "            if unique_id == line[0].strip():\n",
    "                row_els = [word.lower().strip() for word in line[8].split(' ')]\n",
    "                return binary, row_els\n",
    "            else:\n",
    "                continue\n",
    "    print \"URL NOT FOUND IN SOURCE FILE\"\n",
    "    return 0, []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_source = 'data/medical_docs_with_id.csv'\n",
    "weights_path = 'models/som_weights_3'\n",
    "print \"Loading weights\"\n",
    "som.weights = np.load(\"{}.npy\".format(weights_path))\n",
    "\n",
    "import numpy as np\n",
    "# array for empty retinas\n",
    "empty_arr = np.zeros(128*128)\n",
    "retinas_dict = {}\n",
    "\n",
    "print \"Importing from data file\"\n",
    "for id in range(2119):  # since the data file only has 2118 records\n",
    "    unique_id = str(id)\n",
    "#     print unique_id\n",
    "    binary, words = import_from_data_file(unique_id)\n",
    "    if binary not in retinas_dict:\n",
    "        retinas_dict[binary] = {}\n",
    "    arr = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            x, y = som.winner(word_vectors[word])\n",
    "            unit = [x, y]\n",
    "            arr.append(unit)\n",
    "#             print word\n",
    "        except:\n",
    "            pass\n",
    "#     print arr\n",
    "    # convert retinas to arrays\n",
    "    for i in range(len(arr)):\n",
    "#         print arr[i]\n",
    "        index = arr[i][0] + arr[i][1] * 128\n",
    "#         print index\n",
    "        empty_arr[index] = 1\n",
    "    if unique_id not in retinas_dict:\n",
    "        retinas_dict[binary][unique_id] = empty_arr\n",
    "print retinas_dict\n",
    "\n",
    "# print retinas_dict['5']\n",
    "# values = retinas_dict['5']\n",
    "# for i in range(len(values)):\n",
    "#     if values[i] == 1:\n",
    "#         print 'index', i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def import_from_data_file(unique_id):\n",
    "    with open(data_source, 'r') as source:\n",
    "        csvfile = csv.reader(source)\n",
    "        for line in csvfile:\n",
    "            binary = line[6]\n",
    "            if unique_id == line[0].strip():\n",
    "                row_els = [word.lower().strip() for word in line[8].split(' ')]\n",
    "                return binary, row_els\n",
    "            else:\n",
    "                continue\n",
    "    print \"URL NOT FOUND IN SOURCE FILE\"\n",
    "    return 0, []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_source = 'data/medical_docs_with_id.csv'\n",
    "weights_path = 'models/som_weights_3'\n",
    "print \"Loading weights\"\n",
    "som.weights = np.load(\"{}.npy\".format(weights_path))\n",
    "\n",
    "\n",
    "print \"Importing from data file\"\n",
    "for id in range(10):\n",
    "    unique_id = str(id)\n",
    "    print unique_id\n",
    "    binary, words = import_from_data_file(unique_id)\n",
    "    arrx = []\n",
    "    arry = []\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            x, y = som.winner(word_vectors[word])\n",
    "            arrx.append(x)\n",
    "            arry.append(y)\n",
    "            print word, str(x), str(y)\n",
    "        except:\n",
    "            pass\n",
    "    #print \"Plotting figure\"\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(arrx,arry)\n",
    "    plt.axis([0,128,0,128])\n",
    "    plt.grid()\n",
    "    print \"Saving figure to fingerprints/{}.png\".format(unique_id)\n",
    "    plt.title(\"CLASS: {}  unique_id: {}\".format(str(binary), unique_id))\n",
    "    plt.savefig(\"fingerprints/{}.png\".format(unique_id))\n",
    "    #plt.show()\n",
    "    \n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
